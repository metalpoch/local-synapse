# Local Synapse 游

**Local Synapse** es un proxy ligero escrito en Go dise침ado para conectar un servidor local de [Ollama](https://ollama.com/) con el mundo exterior. 

Actualmente, el proyecto se encuentra en una fase pre-MVP, sirviendo como una herramienta de validaci칩n para asegurar que el servidor local es accesible de forma segura y eficiente desde redes externas.

## 游 Estado Actual

Hoy en d칤a, Local Synapse act칰a como un proxy robusto para la API de Chat de Ollama, ofreciendo:
- **Streaming de alta fidelidad**: Soporte para respuestas largas mediante un b칰fer optimizado de 1MB.
- **Eficiencia de recursos**: Cancelaci칩n autom치tica de peticiones a Ollama si el cliente se desconecta.
- **Configuraci칩n simple**: Configurable mediante variables de entorno (`PORT`, `OLLAMA_URL`, `SYSTEM_PROMPT`).

## 游 Instalaci칩n y Uso

### Prerrequisitos
- Go 1.25+
- Ollama corriendo localmente.

### Ejecuci칩n
1. Clona el repositorio.
2. Ejecuta el servidor:
   ```bash
   go run main.go
   ```
### Ejemplo con cURL
Para ver el streaming en tiempo real desde la terminal:
```bash
curl -N "http://localhost:8080/chat?prompt=Expl칤came+Go+en+una+frase"
```
*(El flag `-N` es importante para desactivar el buffering de cURL).*

### Ejemplo con JavaScript (Frontend)
Si quieres consumirlo desde una web:
```javascript
const eventSource = new EventSource('http://localhost:8080/chat?prompt=Hola');

eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);
    console.log(data.message.content); // Aqu칤 llega el fragmento de texto
};

eventSource.onerror = () => {
    eventSource.close();
};
```

### Ejecuci칩n con Podman/Docker
El proyecto se construye autom치ticamente en cada cambio a `main`. Puedes obtener la imagen desde GitHub Packages:
```bash
podman pull ghcr.io/${{ github.repository }}:latest
```

O ejecutarlo directamente con Compose:
```bash
podman-compose up -d
```

## 游댩 Visi칩n a Futuro

Este proyecto no se detendr치 en ser un simple proxy. El objetivo es evolucionar hacia una plataforma integrada que permita:

1.  **Gesti칩n de Proyectos Electr칩nicos**: Una interfaz para visualizar datos de sensores, controlar actuadores y organizar esquem치ticos/documentaci칩n t칠cnica de mis proyectos.
2.  **Asistente LLM Gen칠rico**: Un compa침ero de IA personalizado que no solo responda preguntas, sino que entienda el contexto de mis desarrollos locales.
4.  **Integraci칩n con MCP (Model Context Protocol)**: Actuar como un host o cliente MCP para permitir que el LLM interact칰e din치micamente con herramientas externas y bases de conocimiento.
5.  **Frontend Interactivo**: Un panel de control moderno para visualizar el estado de los proyectos electr칩nicos y chatear con los modelos de forma fluida.
6.  **Puente Hardware-IA**: Utilizar la potencia de los LLMs locales para analizar telemetr칤a de hardware en tiempo real.

---
*Desarrollado con 仇벒잺 por poch.*
